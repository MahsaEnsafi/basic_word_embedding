# -*- coding: utf-8 -*-
"""Word embedding.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tcYISqq09UML11Kkqcz_76LupcQlJSVl
"""

import numpy as np
from tensorflow.keras.preprocessing.text import one_hot
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,Flatten,Embedding

reviews=['nice food','amazing restaurant','too good','just love it','will go again',
         'horrible food','never go there','poor service','poor quality','needs improvement']
sentiments=np.array([1,1,1,1,1,0,0,0,0,0])

vocab_size=30
encoded_reviews=[one_hot(d,vocab_size) for d in reviews]
encoded_reviews

max_length=4
paded_review=pad_sequences(encoded_reviews,maxlen=max_length,padding='post')

embeded_vector_size=4
model=Sequential()
model.add(Embedding(vocab_size,embeded_vector_size,input_length=max_length,name='embedding'))
model.add(Flatten())
model.add(Dense(1,activation='sigmoid'))
model.summary()

X=paded_review
Y=sentiments

model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])
model.fit(X,Y,epochs=1)

weights=model.get_layer('embedding').get_weights()[0]
weights